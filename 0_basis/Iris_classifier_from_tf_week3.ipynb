{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "copyrighted-niagara",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "\n",
    "This programming assignment is migrated from tensorflow 2.0 exercise, aiming at familiar with pytorch programming API\n",
    "\n",
    "Reference:\n",
    "* https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "* https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "* https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "* https://blog.csdn.net/touristourist/article/details/100535544\n",
    "* Transform - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "* Torch wrapper for similar APIs to keras - https://github.com/ncullen93/torchsample\n",
    "* Torch weight and bias initialization - https://androidkt.com/initialize-weight-bias-pytorch/\n",
    "* Torch regularization - https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-first",
   "metadata": {},
   "source": [
    "## Model validation on the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-semiconductor",
   "metadata": {},
   "source": [
    "#### The Iris dataset\n",
    "\n",
    "In this assignment, you will use the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). It consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. For a reference, see the following papers:\n",
    "\n",
    "- R. A. Fisher. \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179â€“188, 1936.\n",
    "\n",
    "Your goal is to construct a neural network that classifies each sample into the correct class, as well as applying validation and regularisation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn import datasets, model_selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-imaging",
   "metadata": {},
   "source": [
    "#### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TensorDataSet(Dataset):\n",
    "    def __init__(self, inputs, targets, \n",
    "                 transform: Optional[Callable]=None,\n",
    "                 target_transform: Optional[Callable]=None):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transforms\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_data, input_target = self.inputs[idx], self.targets[idx]\n",
    "        if self.transform is not None:\n",
    "            input_data = self.transform(input_data)\n",
    "        if self.target_transform is not None:\n",
    "            input_target = self.target_transform(input_target)\n",
    "        return input_data, input_target\n",
    "    \n",
    "class IrisData():\n",
    "    \"\"\" Iris training, validation or testing dataset load from sklearn\n",
    "    \"\"\"\n",
    "    default_batch_size:int=64\n",
    "    \n",
    "    def __init__(self, transforms: Dict[str, Optional[Callable]], target_transforms: Dict[str, Optional[Callable]], \n",
    "                 batch_size: Dict[str, int],\n",
    "                 train_test_split: float=0.15, valid_train_split: float=0.2,):\n",
    "        train_data, train_target, valid_data, valid_target, test_data, test_target = self._read_in_and_split_data(train_test_split, valid_train_split)\n",
    "        self.train_dataset = TensorDataSet(train_data, train_target, \n",
    "                                           transforms.get(\"train\", None), target_transforms.get(\"train\", None))\n",
    "        self.valid_dataset = TensorDataSet(valid_data, valid_target, \n",
    "                                           transforms.get(\"valid\", None), target_transforms.get(\"valid\", None))\n",
    "        self.test_dataset = TensorDataSet(test_data, test_target, \n",
    "                                          transforms.get(\"test\", None), target_transforms.get(\"test\", None))\n",
    "        self.train_loader = DataLoader(self.train_dataset, \n",
    "                                       batch_size=batch_size.get(\"train\", default_batch_size), \n",
    "                                       shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, \n",
    "                                       batch_size=batch_size.get(\"valid\", default_batch_size), \n",
    "                                       shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_dataset, \n",
    "                                      batch_size=batch_size.get(\"test\", default_batch_size), \n",
    "                                      shuffle=True)\n",
    "        \n",
    "    def _read_in_and_split_data(train_test_split, valid_train_split):\n",
    "        iris_data = datasets.load_iris()\n",
    "        tranvalid_data, test_data, tranvalid_target, test_target = train_test_split(iris_data.data, iris_data.target,\n",
    "                                                                                    test_size=train_test_split,\n",
    "                                                                                    random_state=1)\n",
    "        train_data, valid_data, train_target, valid_target = train_test_split(tranvalid_data, tranvalid_target,\n",
    "                                                                              test_size=valid_train_split,\n",
    "                                                                              random_state=1)\n",
    "        return train_data, train_target, valid_data, valid_target, test_data, test_target\n",
    "    \n",
    "    def get_train(self):\n",
    "        return self.train_loader, self.train_dataset\n",
    "    \n",
    "    def get_valid(self):\n",
    "        return self.valid_loader, self.valid_dataset\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.test_loader, self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate: float):\n",
    "        super(RegularizedNetwork, self).__init__()\n",
    "        self.first_linear_layer = nn.LazyLinear(64)\n",
    "        self.regularized_stack = nn.Sequential(\n",
    "            self.first_linear_layer, nn.ReLU(),\n",
    "            nn.LazyLinear(128), nn.ReLU(),\n",
    "            nn.LazyLinear(128), nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.LazyLinear(128), nn.ReLU(),\n",
    "            nn.LazyLinear(128), nn.ReLU(),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LazyLinear(64), nn.ReLU(),\n",
    "            nn.LazyLinear(64), nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.LazyLinear(64), nn.ReLU(),\n",
    "            nn.LazyLinear(64), nn.ReLU(),\n",
    "            nn.LazyLinear(3),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        # intialization for weight and bias\n",
    "        nn.init.kaiming_uniform_(self.first_linear_layer.weight.data)\n",
    "        nn.init.constant_(self.first_linear_layer.bias.data, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.regularized_stack(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

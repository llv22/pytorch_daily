{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corporate-deadline",
   "metadata": {},
   "source": [
    "# Itâ€™s NeRF From Nothing: Build A Complete NeRF with PyTorch\n",
    "Reference:\n",
    "* https://towardsdatascience.com/its-nerf-from-nothing-build-a-vanilla-nerf-with-pytorch-7846e4c45666\n",
    "* [Code colab](https://colab.research.google.com/drive/1TppdSsLz8uKoNwqJqDGg8se8BHQcvg_K?usp=sharing)\n",
    "\n",
    "This notebook walks the reader through a full implementation of the original Neural Radiance Field architecture, first introduced by Mildenhall et al. in \"[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.](https://www.matthewtancik.com/nerf)\" For a broader overview, read the accompanying Medium article \"[It's NeRF From Nothing: Build A Complete NeRF With Pytorch.](https://medium.com/@masonmcgough/its-nerf-from-nothing-build-a-vanilla-nerf-with-pytorch-7846e4c45666)\" This notebook assumes that you have read that article and understand the basics of NeRF.\n",
    "\n",
    "Much of the code comes from or is inspired by the original implementation by GitHub user [bmild](https://github.com/bmild/nerf) as well as PyTorch implementations from GitHub users [yenchenlin](https://github.com/bmild/nerf) and [krrish94](https://github.com/krrish94/nerf-pytorch/). The code has been modified for clarity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indirect-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple, Optional## Differentiable Volume Renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-oasis",
   "metadata": {},
   "source": [
    "## Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "  r\"\"\"\n",
    "  Sine-cosine positional encoder for input points.\n",
    "  \"\"\"\n",
    "  def __init__(self, d_input: int, n_freqs: int, log_space: bool = False):\n",
    "    super().__init__()\n",
    "    self.d_input = d_input\n",
    "    self.n_freqs = n_freqs\n",
    "    self.log_space = log_space\n",
    "    self.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "    self.embed_fns = [lambda x: x]\n",
    "\n",
    "    # Define frequencies in either linear or log scale\n",
    "    if self.log_space:\n",
    "      freq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "    else:\n",
    "      freq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "    # Alternate sin and cos\n",
    "    for freq in freq_bands:\n",
    "      self.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "      self.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "  \n",
    "  def forward(self, x) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Apply positional encoding to input.\n",
    "    \"\"\"\n",
    "    return torch.concat([fn(x) for fn in self.embed_fns], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-leader",
   "metadata": {},
   "source": [
    "## NeRF Function $F:(x,d) \\rightarrow (c:RGB, \\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "positive-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "  r\"\"\"\n",
    "  Neural radiance fields module.\n",
    "  \"\"\"\n",
    "  def __init__(self, d_input: int = 3, n_layers: int = 8, d_filter: int = 256, skip: Tuple[int] = (4,), d_viewdirs: Optional[int] = None):\n",
    "    super().__init__()\n",
    "    self.d_input = d_input\n",
    "    self.skip = skip\n",
    "    self.act = nn.functional.relu\n",
    "    self.d_viewdirs = d_viewdirs\n",
    "\n",
    "    # Create model layers\n",
    "    self.layers = nn.ModuleList(\n",
    "      [nn.Linear(self.d_input, d_filter)] +\n",
    "      [nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "       else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "    )\n",
    "\n",
    "    # Bottleneck layers\n",
    "    if self.d_viewdirs is not None:\n",
    "      # If using viewdirs, split alpha and RGB\n",
    "      self.alpha_out = nn.Linear(d_filter, 1)\n",
    "      self.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "      self.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "      self.output = nn.Linear(d_filter // 2, 3)\n",
    "    else:\n",
    "      # If no viewdirs, use simpler output\n",
    "      self.output = nn.Linear(d_filter, 4)\n",
    "  \n",
    "  def forward(self, x: torch.Tensor, viewdirs: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Forward pass with optional view direction.\n",
    "    \"\"\"\n",
    "    # Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "    if self.d_viewdirs is None and viewdirs is not None:\n",
    "      raise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "    # Apply forward pass up to bottleneck\n",
    "    x_input = x\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      x = self.act(layer(x))\n",
    "      if i in self.skip:\n",
    "        x = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "    # Apply bottleneck\n",
    "    if self.d_viewdirs is not None:\n",
    "      # Split alpha from network output\n",
    "      alpha = self.alpha_out(x)\n",
    "\n",
    "      # Pass through bottleneck to get RGB\n",
    "      x = self.rgb_filters(x)\n",
    "      x = torch.concat([x, viewdirs], dim=-1)\n",
    "      x = self.act(self.branch(x))\n",
    "      x = self.output(x)\n",
    "\n",
    "      # Concatenate alphas to output\n",
    "      x = torch.concat([x, alpha], dim=-1)\n",
    "    else:\n",
    "      # Simple output\n",
    "      x = self.output(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-assault",
   "metadata": {},
   "source": [
    "## Differentiable Volume Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incredible-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw2outputs(raw: torch.Tensor, z_vals: torch.Tensor, rays_d: torch.Tensor, raw_noise_std: float = 0.0, white_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "  r\"\"\"\n",
    "  Convert the raw NeRF output into RGB and other maps.\n",
    "  \"\"\"\n",
    "  # Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "  dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "  dists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "  # Multiply each distance by the norm of its corresponding direction ray\n",
    "  # to convert to real world distance (accounts for non-unit directions).\n",
    "  dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "  # Add noise to model's predictions for density. Can be used to \n",
    "  # regularize network during training (prevents floater artifacts).\n",
    "  noise = 0.\n",
    "  if raw_noise_std > 0.:\n",
    "    noise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "  # Predict density of each sample along each ray. Higher values imply\n",
    "  # higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "  alpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
    "\n",
    "  # Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "  # The higher the alpha, the lower subsequent weights are driven.\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "  # Compute weighted RGB map.\n",
    "  rgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "  rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "  # Estimated depth map is predicted distance.\n",
    "  depth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "  # Disparity map is inverse depth.\n",
    "  disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n",
    "\n",
    "  # Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "  acc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "  # To composite onto a white background, use the accumulated alpha map.\n",
    "  if white_bkgd:\n",
    "    rgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "  return rgb_map, depth_map, acc_map, weights\n",
    "\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "  r\"\"\"\n",
    "  (Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "  Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "  Args:\n",
    "  tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "    is to be computed.\n",
    "  Returns:\n",
    "  cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "    tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "  \"\"\"\n",
    "\n",
    "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "  cumprod = torch.cumprod(tensor, -1)\n",
    "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "  cumprod = torch.roll(cumprod, 1, -1)\n",
    "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "  cumprod[..., 0] = 1.\n",
    "  \n",
    "  return cumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-endorsement",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stratified(rays_o: torch.Tensor, rays_d: torch.Tensor, near: float, far: float, n_samples: int, \n",
    "                      perturb: Optional[bool] = True, inverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  r\"\"\"\n",
    "  Sample along ray from regularly-spaced bins.\n",
    "  \"\"\"\n",
    "  # Grab samples for space integration along ray\n",
    "  t_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "  if not inverse_depth:\n",
    "    # Sample linearly between `near` and `far`\n",
    "    z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "  else:\n",
    "    # Sample linearly in inverse depth (disparity)\n",
    "    z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "  # Draw uniform samples from bins along ray\n",
    "  if perturb:\n",
    "    mids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "    upper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "    lower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "    t_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "    z_vals = lower + (upper - lower) * t_rand\n",
    "  z_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "  # Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "  # pts: (width, height, n_samples, 3)\n",
    "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "  return pts, z_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-fifteen",
   "metadata": {},
   "source": [
    "## Hierarchical Volume Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hierarchical(rays_o: torch.Tensor, rays_d: torch.Tensor, z_vals: torch.Tensor,\n",
    "                        weights: torch.Tensor, n_samples: int, perturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "  r\"\"\"\n",
    "  Apply hierarchical sampling to the rays.\n",
    "  \"\"\"\n",
    "\n",
    "  # Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "  z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "  new_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples, perturb=perturb)\n",
    "  new_z_samples = new_z_samples.detach()\n",
    "\n",
    "  # Resample points from ray based on PDF.\n",
    "  z_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "  pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "  return pts, z_vals_combined, new_z_samples\n",
    "\n",
    "\n",
    "def sample_pdf(bins: torch.Tensor, weights: torch.Tensor, n_samples: int, perturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "  r\"\"\"\n",
    "  Apply inverse transform sampling to a weighted set of points.\n",
    "  \"\"\"\n",
    "\n",
    "  # Normalize weights to get PDF.\n",
    "  pdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "  # Convert PDF to CDF.\n",
    "  cdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "  cdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "  # Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "  if not perturb:\n",
    "    u = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "    u = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "  else:\n",
    "    u = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "  # Find indices along CDF where values in u would be placed.\n",
    "  u = u.contiguous() # Returns contiguous tensor with same values.\n",
    "  inds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "  # Clamp indices that are out of bounds.\n",
    "  below = torch.clamp(inds - 1, min=0)\n",
    "  above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "  inds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "  # Sample from cdf and the corresponding bin centers.\n",
    "  matched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "  cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1, index=inds_g)\n",
    "  bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1, index=inds_g)\n",
    "\n",
    "  # Convert samples to ray length.\n",
    "  denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "  denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "  t = (u - cdf_g[..., 0]) / denom\n",
    "  samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "  return samples # [n_rays, n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-affiliation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

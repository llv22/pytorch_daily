{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6bef47",
   "metadata": {},
   "source": [
    "# Transformer with standard decoder\n",
    "\n",
    "* Reference: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1067864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -int, with zeros on dialog\n",
    "    refer to https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float=0.1, max_len: int=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        # here we also add a dropout layer for position encoding\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Tansformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        # should be replaced with TransformerDeCoder\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len\n",
    "            \n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb05c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk'])\n",
    "vocab.set_default_index(vocab['<unk'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"\n",
    "    converts raw text into a flat tensor\n",
    "    \"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab, so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides the data into bsz separate sequences, removing extra elements that wouldn't clearly fit.\n",
    "    \n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    # rounding up based on batch size\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    # transform to bsz * seq_len, then use contiguous() to return a contiguous in memory tensor \n",
    "    # containing the same data as self tensor. If self tensor is already in the specified memory format, \n",
    "    # this function returns the self tensor: refer to https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html?highlight=contiguous#torch.Tensor.contiguous\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size) # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585a8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sources: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "        \n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f07cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc0648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65b5bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 15.72 | loss  8.19 | ppl  3613.34\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 14.49 | loss  6.90 | ppl   994.94\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.44 | ppl   627.55\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.30 | ppl   545.70\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.19 | ppl   489.58\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.16 | ppl   472.29\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.12 | ppl   456.12\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 14.50 | loss  6.11 | ppl   451.29\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  6.03 | ppl   415.73\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 14.89 | loss  6.02 | ppl   412.03\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 14.52 | loss  5.89 | ppl   361.80\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 14.50 | loss  5.97 | ppl   392.97\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 14.52 | loss  5.95 | ppl   385.06\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 14.51 | loss  5.88 | ppl   357.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 44.71s | valid loss  5.87 | valid ppl   352.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 14.70 | loss  5.87 | ppl   354.90\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.85 | ppl   347.71\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.67 | ppl   289.07\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 14.56 | loss  5.71 | ppl   300.96\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.66 | ppl   286.68\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 14.54 | loss  5.69 | ppl   295.24\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 14.54 | loss  5.69 | ppl   297.35\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.54 | loss  5.72 | ppl   303.88\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.66 | ppl   287.60\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.67 | ppl   289.91\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.56 | ppl   259.64\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.65 | ppl   285.57\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 14.54 | loss  5.65 | ppl   285.15\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.55 | loss  5.59 | ppl   268.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 44.54s | valid loss  5.64 | valid ppl   282.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 14.70 | loss  5.61 | ppl   273.28\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.64 | ppl   281.07\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.43 | ppl   228.46\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.55 | loss  5.49 | ppl   241.82\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.45 | ppl   232.00\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 14.55 | loss  5.48 | ppl   240.74\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 14.63 | loss  5.50 | ppl   245.35\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.53 | ppl   251.63\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 14.57 | loss  5.48 | ppl   238.65\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.49 | ppl   242.66\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 14.55 | loss  5.37 | ppl   214.68\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.47 | ppl   237.56\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.48 | ppl   239.29\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.56 | loss  5.40 | ppl   222.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 44.57s | valid loss  5.60 | valid ppl   270.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 4.29 | ms/batch 14.72 | loss  5.44 | ppl   230.15\n",
      "| epoch   4 |   400/ 2928 batches | lr 4.29 | ms/batch 14.56 | loss  5.47 | ppl   237.11\n",
      "| epoch   4 |   600/ 2928 batches | lr 4.29 | ms/batch 14.56 | loss  5.27 | ppl   194.05\n",
      "| epoch   4 |   800/ 2928 batches | lr 4.29 | ms/batch 14.58 | loss  5.33 | ppl   206.53\n",
      "| epoch   4 |  1000/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.28 | ppl   197.05\n",
      "| epoch   4 |  1200/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.33 | ppl   206.62\n",
      "| epoch   4 |  1400/ 2928 batches | lr 4.29 | ms/batch 14.59 | loss  5.35 | ppl   209.87\n",
      "| epoch   4 |  1600/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.38 | ppl   217.21\n",
      "| epoch   4 |  1800/ 2928 batches | lr 4.29 | ms/batch 14.58 | loss  5.33 | ppl   206.45\n",
      "| epoch   4 |  2000/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.35 | ppl   210.11\n",
      "| epoch   4 |  2200/ 2928 batches | lr 4.29 | ms/batch 14.59 | loss  5.21 | ppl   183.52\n",
      "| epoch   4 |  2400/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.32 | ppl   203.94\n",
      "| epoch   4 |  2600/ 2928 batches | lr 4.29 | ms/batch 14.57 | loss  5.33 | ppl   206.61\n",
      "| epoch   4 |  2800/ 2928 batches | lr 4.29 | ms/batch 14.59 | loss  5.27 | ppl   193.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 44.77s | valid loss  5.54 | valid ppl   255.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 4.07 | ms/batch 14.78 | loss  5.31 | ppl   201.37\n",
      "| epoch   5 |   400/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.34 | ppl   208.22\n",
      "| epoch   5 |   600/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.14 | ppl   169.94\n",
      "| epoch   5 |   800/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.20 | ppl   181.70\n",
      "| epoch   5 |  1000/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.16 | ppl   174.22\n",
      "| epoch   5 |  1200/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.22 | ppl   184.94\n",
      "| epoch   5 |  1400/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.23 | ppl   187.01\n",
      "| epoch   5 |  1600/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.26 | ppl   192.58\n",
      "| epoch   5 |  1800/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.22 | ppl   185.84\n",
      "| epoch   5 |  2000/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.23 | ppl   186.98\n",
      "| epoch   5 |  2200/ 2928 batches | lr 4.07 | ms/batch 14.59 | loss  5.09 | ppl   162.94\n",
      "| epoch   5 |  2400/ 2928 batches | lr 4.07 | ms/batch 14.58 | loss  5.22 | ppl   184.86\n",
      "| epoch   5 |  2600/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.22 | ppl   185.31\n",
      "| epoch   5 |  2800/ 2928 batches | lr 4.07 | ms/batch 14.57 | loss  5.15 | ppl   173.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 44.63s | valid loss  5.51 | valid ppl   248.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 3.87 | ms/batch 14.74 | loss  5.20 | ppl   180.59\n",
      "| epoch   6 |   400/ 2928 batches | lr 3.87 | ms/batch 14.72 | loss  5.23 | ppl   187.19\n",
      "| epoch   6 |   600/ 2928 batches | lr 3.87 | ms/batch 14.79 | loss  5.03 | ppl   153.37\n",
      "| epoch   6 |   800/ 2928 batches | lr 3.87 | ms/batch 14.74 | loss  5.11 | ppl   164.93\n",
      "| epoch   6 |  1000/ 2928 batches | lr 3.87 | ms/batch 14.68 | loss  5.06 | ppl   157.69\n",
      "| epoch   6 |  1200/ 2928 batches | lr 3.87 | ms/batch 14.69 | loss  5.11 | ppl   166.19\n",
      "| epoch   6 |  1400/ 2928 batches | lr 3.87 | ms/batch 14.64 | loss  5.13 | ppl   169.12\n",
      "| epoch   6 |  1600/ 2928 batches | lr 3.87 | ms/batch 14.61 | loss  5.17 | ppl   175.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1800/ 2928 batches | lr 3.87 | ms/batch 14.60 | loss  5.12 | ppl   168.04\n",
      "| epoch   6 |  2000/ 2928 batches | lr 3.87 | ms/batch 14.58 | loss  5.13 | ppl   168.34\n",
      "| epoch   6 |  2200/ 2928 batches | lr 3.87 | ms/batch 14.57 | loss  5.00 | ppl   148.11\n",
      "| epoch   6 |  2400/ 2928 batches | lr 3.87 | ms/batch 14.58 | loss  5.11 | ppl   165.72\n",
      "| epoch   6 |  2600/ 2928 batches | lr 3.87 | ms/batch 14.57 | loss  5.12 | ppl   166.67\n",
      "| epoch   6 |  2800/ 2928 batches | lr 3.87 | ms/batch 14.62 | loss  5.07 | ppl   158.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 44.78s | valid loss  5.53 | valid ppl   251.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 3.68 | ms/batch 14.78 | loss  5.10 | ppl   163.99\n",
      "| epoch   7 |   400/ 2928 batches | lr 3.68 | ms/batch 14.63 | loss  5.13 | ppl   169.59\n",
      "| epoch   7 |   600/ 2928 batches | lr 3.68 | ms/batch 14.64 | loss  4.94 | ppl   140.18\n",
      "| epoch   7 |   800/ 2928 batches | lr 3.68 | ms/batch 14.64 | loss  5.01 | ppl   149.29\n",
      "| epoch   7 |  1000/ 2928 batches | lr 3.68 | ms/batch 14.63 | loss  4.97 | ppl   143.90\n",
      "| epoch   7 |  1200/ 2928 batches | lr 3.68 | ms/batch 14.65 | loss  5.03 | ppl   152.51\n",
      "| epoch   7 |  1400/ 2928 batches | lr 3.68 | ms/batch 14.67 | loss  5.04 | ppl   154.91\n",
      "| epoch   7 |  1600/ 2928 batches | lr 3.68 | ms/batch 14.66 | loss  5.08 | ppl   160.85\n",
      "| epoch   7 |  1800/ 2928 batches | lr 3.68 | ms/batch 14.67 | loss  5.04 | ppl   154.75\n",
      "| epoch   7 |  2000/ 2928 batches | lr 3.68 | ms/batch 14.66 | loss  5.04 | ppl   155.24\n",
      "| epoch   7 |  2200/ 2928 batches | lr 3.68 | ms/batch 14.67 | loss  4.91 | ppl   135.57\n",
      "| epoch   7 |  2400/ 2928 batches | lr 3.68 | ms/batch 14.64 | loss  5.02 | ppl   152.08\n",
      "| epoch   7 |  2600/ 2928 batches | lr 3.68 | ms/batch 14.63 | loss  5.04 | ppl   153.90\n",
      "| epoch   7 |  2800/ 2928 batches | lr 3.68 | ms/batch 14.71 | loss  4.97 | ppl   144.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 44.82s | valid loss  5.53 | valid ppl   252.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 3.49 | ms/batch 14.98 | loss  5.01 | ppl   149.65\n",
      "| epoch   8 |   400/ 2928 batches | lr 3.49 | ms/batch 14.81 | loss  5.04 | ppl   155.14\n",
      "| epoch   8 |   600/ 2928 batches | lr 3.49 | ms/batch 14.79 | loss  4.87 | ppl   129.94\n",
      "| epoch   8 |   800/ 2928 batches | lr 3.49 | ms/batch 14.71 | loss  4.93 | ppl   137.77\n",
      "| epoch   8 |  1000/ 2928 batches | lr 3.49 | ms/batch 14.72 | loss  4.90 | ppl   133.72\n",
      "| epoch   8 |  1200/ 2928 batches | lr 3.49 | ms/batch 14.71 | loss  4.96 | ppl   141.91\n",
      "| epoch   8 |  1400/ 2928 batches | lr 3.49 | ms/batch 14.71 | loss  4.96 | ppl   142.72\n",
      "| epoch   8 |  1600/ 2928 batches | lr 3.49 | ms/batch 14.71 | loss  5.01 | ppl   149.21\n",
      "| epoch   8 |  1800/ 2928 batches | lr 3.49 | ms/batch 14.66 | loss  4.97 | ppl   143.52\n",
      "| epoch   8 |  2000/ 2928 batches | lr 3.49 | ms/batch 14.66 | loss  4.98 | ppl   145.01\n",
      "| epoch   8 |  2200/ 2928 batches | lr 3.49 | ms/batch 14.78 | loss  4.83 | ppl   125.37\n",
      "| epoch   8 |  2400/ 2928 batches | lr 3.49 | ms/batch 14.75 | loss  4.94 | ppl   139.80\n",
      "| epoch   8 |  2600/ 2928 batches | lr 3.49 | ms/batch 14.73 | loss  4.95 | ppl   141.88\n",
      "| epoch   8 |  2800/ 2928 batches | lr 3.49 | ms/batch 14.71 | loss  4.90 | ppl   134.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 45.08s | valid loss  5.49 | valid ppl   242.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 3.32 | ms/batch 14.72 | loss  4.94 | ppl   139.85\n",
      "| epoch   9 |   400/ 2928 batches | lr 3.32 | ms/batch 14.58 | loss  4.97 | ppl   144.34\n",
      "| epoch   9 |   600/ 2928 batches | lr 3.32 | ms/batch 14.57 | loss  4.79 | ppl   119.77\n",
      "| epoch   9 |   800/ 2928 batches | lr 3.32 | ms/batch 14.57 | loss  4.85 | ppl   127.69\n",
      "| epoch   9 |  1000/ 2928 batches | lr 3.32 | ms/batch 14.70 | loss  4.82 | ppl   124.53\n",
      "| epoch   9 |  1200/ 2928 batches | lr 3.32 | ms/batch 14.64 | loss  4.88 | ppl   131.53\n",
      "| epoch   9 |  1400/ 2928 batches | lr 3.32 | ms/batch 14.81 | loss  4.89 | ppl   132.57\n",
      "| epoch   9 |  1600/ 2928 batches | lr 3.32 | ms/batch 14.82 | loss  4.93 | ppl   138.40\n",
      "| epoch   9 |  1800/ 2928 batches | lr 3.32 | ms/batch 14.82 | loss  4.90 | ppl   133.75\n",
      "| epoch   9 |  2000/ 2928 batches | lr 3.32 | ms/batch 14.81 | loss  4.90 | ppl   134.21\n",
      "| epoch   9 |  2200/ 2928 batches | lr 3.32 | ms/batch 14.79 | loss  4.76 | ppl   116.43\n",
      "| epoch   9 |  2400/ 2928 batches | lr 3.32 | ms/batch 14.70 | loss  4.87 | ppl   130.40\n",
      "| epoch   9 |  2600/ 2928 batches | lr 3.32 | ms/batch 14.70 | loss  4.88 | ppl   131.91\n",
      "| epoch   9 |  2800/ 2928 batches | lr 3.32 | ms/batch 14.70 | loss  4.82 | ppl   124.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 44.98s | valid loss  5.52 | valid ppl   250.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 3.15 | ms/batch 14.81 | loss  4.87 | ppl   129.96\n",
      "| epoch  10 |   400/ 2928 batches | lr 3.15 | ms/batch 14.64 | loss  4.90 | ppl   134.46\n",
      "| epoch  10 |   600/ 2928 batches | lr 3.15 | ms/batch 14.63 | loss  4.72 | ppl   112.32\n",
      "| epoch  10 |   800/ 2928 batches | lr 3.15 | ms/batch 14.71 | loss  4.78 | ppl   119.58\n",
      "| epoch  10 |  1000/ 2928 batches | lr 3.15 | ms/batch 14.83 | loss  4.76 | ppl   116.40\n",
      "| epoch  10 |  1200/ 2928 batches | lr 3.15 | ms/batch 14.83 | loss  4.81 | ppl   122.90\n",
      "| epoch  10 |  1400/ 2928 batches | lr 3.15 | ms/batch 14.73 | loss  4.82 | ppl   124.42\n",
      "| epoch  10 |  1600/ 2928 batches | lr 3.15 | ms/batch 14.68 | loss  4.86 | ppl   129.65\n",
      "| epoch  10 |  1800/ 2928 batches | lr 3.15 | ms/batch 14.67 | loss  4.83 | ppl   125.21\n",
      "| epoch  10 |  2000/ 2928 batches | lr 3.15 | ms/batch 14.69 | loss  4.84 | ppl   125.85\n",
      "| epoch  10 |  2200/ 2928 batches | lr 3.15 | ms/batch 14.68 | loss  4.69 | ppl   108.72\n",
      "| epoch  10 |  2400/ 2928 batches | lr 3.15 | ms/batch 14.73 | loss  4.80 | ppl   121.48\n",
      "| epoch  10 |  2600/ 2928 batches | lr 3.15 | ms/batch 14.64 | loss  4.82 | ppl   123.81\n",
      "| epoch  10 |  2800/ 2928 batches | lr 3.15 | ms/batch 14.63 | loss  4.76 | ppl   116.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 44.93s | valid loss  5.54 | valid ppl   253.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2928 batches | lr 2.99 | ms/batch 14.80 | loss  4.81 | ppl   122.55\n",
      "| epoch  11 |   400/ 2928 batches | lr 2.99 | ms/batch 14.78 | loss  4.84 | ppl   126.10\n",
      "| epoch  11 |   600/ 2928 batches | lr 2.99 | ms/batch 14.81 | loss  4.66 | ppl   105.97\n",
      "| epoch  11 |   800/ 2928 batches | lr 2.99 | ms/batch 14.81 | loss  4.73 | ppl   112.79\n",
      "| epoch  11 |  1000/ 2928 batches | lr 2.99 | ms/batch 14.79 | loss  4.70 | ppl   110.43\n",
      "| epoch  11 |  1200/ 2928 batches | lr 2.99 | ms/batch 14.79 | loss  4.76 | ppl   116.47\n",
      "| epoch  11 |  1400/ 2928 batches | lr 2.99 | ms/batch 14.79 | loss  4.76 | ppl   117.13\n",
      "| epoch  11 |  1600/ 2928 batches | lr 2.99 | ms/batch 14.75 | loss  4.81 | ppl   122.27\n",
      "| epoch  11 |  1800/ 2928 batches | lr 2.99 | ms/batch 14.73 | loss  4.78 | ppl   118.60\n",
      "| epoch  11 |  2000/ 2928 batches | lr 2.99 | ms/batch 14.73 | loss  4.78 | ppl   119.06\n",
      "| epoch  11 |  2200/ 2928 batches | lr 2.99 | ms/batch 14.75 | loss  4.63 | ppl   102.57\n",
      "| epoch  11 |  2400/ 2928 batches | lr 2.99 | ms/batch 14.73 | loss  4.74 | ppl   114.99\n",
      "| epoch  11 |  2600/ 2928 batches | lr 2.99 | ms/batch 14.68 | loss  4.76 | ppl   116.48\n",
      "| epoch  11 |  2800/ 2928 batches | lr 2.99 | ms/batch 14.67 | loss  4.70 | ppl   110.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 45.11s | valid loss  5.54 | valid ppl   253.76\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   200/ 2928 batches | lr 2.84 | ms/batch 14.83 | loss  4.75 | ppl   115.95\n",
      "| epoch  12 |   400/ 2928 batches | lr 2.84 | ms/batch 14.66 | loss  4.78 | ppl   118.56\n",
      "| epoch  12 |   600/ 2928 batches | lr 2.84 | ms/batch 14.69 | loss  4.61 | ppl   100.33\n",
      "| epoch  12 |   800/ 2928 batches | lr 2.84 | ms/batch 14.68 | loss  4.67 | ppl   106.47\n",
      "| epoch  12 |  1000/ 2928 batches | lr 2.84 | ms/batch 14.67 | loss  4.65 | ppl   105.10\n",
      "| epoch  12 |  1200/ 2928 batches | lr 2.84 | ms/batch 14.68 | loss  4.70 | ppl   110.46\n",
      "| epoch  12 |  1400/ 2928 batches | lr 2.84 | ms/batch 14.80 | loss  4.71 | ppl   110.65\n",
      "| epoch  12 |  1600/ 2928 batches | lr 2.84 | ms/batch 14.79 | loss  4.75 | ppl   115.61\n",
      "| epoch  12 |  1800/ 2928 batches | lr 2.84 | ms/batch 14.78 | loss  4.72 | ppl   111.95\n",
      "| epoch  12 |  2000/ 2928 batches | lr 2.84 | ms/batch 14.79 | loss  4.73 | ppl   112.82\n",
      "| epoch  12 |  2200/ 2928 batches | lr 2.84 | ms/batch 14.80 | loss  4.58 | ppl    97.14\n",
      "| epoch  12 |  2400/ 2928 batches | lr 2.84 | ms/batch 14.84 | loss  4.69 | ppl   108.61\n",
      "| epoch  12 |  2600/ 2928 batches | lr 2.84 | ms/batch 14.84 | loss  4.70 | ppl   110.08\n",
      "| epoch  12 |  2800/ 2928 batches | lr 2.84 | ms/batch 14.75 | loss  4.65 | ppl   104.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 45.12s | valid loss  5.52 | valid ppl   248.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2928 batches | lr 2.70 | ms/batch 14.89 | loss  4.69 | ppl   109.16\n",
      "| epoch  13 |   400/ 2928 batches | lr 2.70 | ms/batch 14.74 | loss  4.72 | ppl   112.14\n",
      "| epoch  13 |   600/ 2928 batches | lr 2.70 | ms/batch 14.74 | loss  4.55 | ppl    94.69\n",
      "| epoch  13 |   800/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.62 | ppl   101.27\n",
      "| epoch  13 |  1000/ 2928 batches | lr 2.70 | ms/batch 14.70 | loss  4.60 | ppl    99.49\n",
      "| epoch  13 |  1200/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.65 | ppl   105.03\n",
      "| epoch  13 |  1400/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.65 | ppl   104.76\n",
      "| epoch  13 |  1600/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.70 | ppl   109.69\n",
      "| epoch  13 |  1800/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.67 | ppl   107.07\n",
      "| epoch  13 |  2000/ 2928 batches | lr 2.70 | ms/batch 14.69 | loss  4.67 | ppl   106.99\n",
      "| epoch  13 |  2200/ 2928 batches | lr 2.70 | ms/batch 14.70 | loss  4.53 | ppl    92.59\n",
      "| epoch  13 |  2400/ 2928 batches | lr 2.70 | ms/batch 14.71 | loss  4.64 | ppl   103.43\n",
      "| epoch  13 |  2600/ 2928 batches | lr 2.70 | ms/batch 14.98 | loss  4.65 | ppl   104.78\n",
      "| epoch  13 |  2800/ 2928 batches | lr 2.70 | ms/batch 14.99 | loss  4.60 | ppl    99.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 45.13s | valid loss  5.54 | valid ppl   254.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2928 batches | lr 2.57 | ms/batch 14.90 | loss  4.65 | ppl   104.29\n",
      "| epoch  14 |   400/ 2928 batches | lr 2.57 | ms/batch 14.75 | loss  4.67 | ppl   106.95\n",
      "| epoch  14 |   600/ 2928 batches | lr 2.57 | ms/batch 14.75 | loss  4.51 | ppl    90.73\n",
      "| epoch  14 |   800/ 2928 batches | lr 2.57 | ms/batch 14.70 | loss  4.57 | ppl    96.63\n",
      "| epoch  14 |  1000/ 2928 batches | lr 2.57 | ms/batch 14.70 | loss  4.56 | ppl    95.15\n",
      "| epoch  14 |  1200/ 2928 batches | lr 2.57 | ms/batch 14.67 | loss  4.61 | ppl   100.14\n",
      "| epoch  14 |  1400/ 2928 batches | lr 2.57 | ms/batch 14.66 | loss  4.61 | ppl   100.09\n",
      "| epoch  14 |  1600/ 2928 batches | lr 2.57 | ms/batch 14.66 | loss  4.65 | ppl   104.62\n",
      "| epoch  14 |  1800/ 2928 batches | lr 2.57 | ms/batch 14.80 | loss  4.63 | ppl   102.65\n",
      "| epoch  14 |  2000/ 2928 batches | lr 2.57 | ms/batch 14.79 | loss  4.63 | ppl   102.15\n",
      "| epoch  14 |  2200/ 2928 batches | lr 2.57 | ms/batch 14.81 | loss  4.48 | ppl    88.48\n",
      "| epoch  14 |  2400/ 2928 batches | lr 2.57 | ms/batch 14.80 | loss  4.59 | ppl    98.51\n",
      "| epoch  14 |  2600/ 2928 batches | lr 2.57 | ms/batch 14.79 | loss  4.60 | ppl    99.94\n",
      "| epoch  14 |  2800/ 2928 batches | lr 2.57 | ms/batch 14.80 | loss  4.55 | ppl    95.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 45.12s | valid loss  5.54 | valid ppl   254.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2928 batches | lr 2.44 | ms/batch 14.92 | loss  4.61 | ppl   100.24\n",
      "| epoch  15 |   400/ 2928 batches | lr 2.44 | ms/batch 14.79 | loss  4.62 | ppl   101.87\n",
      "| epoch  15 |   600/ 2928 batches | lr 2.44 | ms/batch 14.75 | loss  4.46 | ppl    86.66\n",
      "| epoch  15 |   800/ 2928 batches | lr 2.44 | ms/batch 14.75 | loss  4.53 | ppl    92.46\n",
      "| epoch  15 |  1000/ 2928 batches | lr 2.44 | ms/batch 14.73 | loss  4.52 | ppl    91.45\n",
      "| epoch  15 |  1200/ 2928 batches | lr 2.44 | ms/batch 14.74 | loss  4.56 | ppl    95.85\n",
      "| epoch  15 |  1400/ 2928 batches | lr 2.44 | ms/batch 14.74 | loss  4.56 | ppl    95.48\n",
      "| epoch  15 |  1600/ 2928 batches | lr 2.44 | ms/batch 14.73 | loss  4.61 | ppl   100.47\n",
      "| epoch  15 |  1800/ 2928 batches | lr 2.44 | ms/batch 14.74 | loss  4.59 | ppl    98.41\n",
      "| epoch  15 |  2000/ 2928 batches | lr 2.44 | ms/batch 14.69 | loss  4.58 | ppl    97.74\n",
      "| epoch  15 |  2200/ 2928 batches | lr 2.44 | ms/batch 14.79 | loss  4.44 | ppl    84.75\n",
      "| epoch  15 |  2400/ 2928 batches | lr 2.44 | ms/batch 14.79 | loss  4.54 | ppl    94.07\n",
      "| epoch  15 |  2600/ 2928 batches | lr 2.44 | ms/batch 14.77 | loss  4.56 | ppl    96.04\n",
      "| epoch  15 |  2800/ 2928 batches | lr 2.44 | ms/batch 14.77 | loss  4.51 | ppl    90.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 45.13s | valid loss  5.53 | valid ppl   251.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2928 batches | lr 2.32 | ms/batch 14.86 | loss  4.56 | ppl    95.77\n",
      "| epoch  16 |   400/ 2928 batches | lr 2.32 | ms/batch 14.70 | loss  4.58 | ppl    97.87\n",
      "| epoch  16 |   600/ 2928 batches | lr 2.32 | ms/batch 14.72 | loss  4.42 | ppl    82.86\n",
      "| epoch  16 |   800/ 2928 batches | lr 2.32 | ms/batch 14.72 | loss  4.49 | ppl    89.26\n",
      "| epoch  16 |  1000/ 2928 batches | lr 2.32 | ms/batch 14.70 | loss  4.47 | ppl    87.54\n",
      "| epoch  16 |  1200/ 2928 batches | lr 2.32 | ms/batch 14.71 | loss  4.52 | ppl    91.74\n",
      "| epoch  16 |  1400/ 2928 batches | lr 2.32 | ms/batch 14.71 | loss  4.52 | ppl    91.96\n",
      "| epoch  16 |  1600/ 2928 batches | lr 2.32 | ms/batch 14.71 | loss  4.57 | ppl    96.17\n",
      "| epoch  16 |  1800/ 2928 batches | lr 2.32 | ms/batch 14.72 | loss  4.54 | ppl    93.96\n",
      "| epoch  16 |  2000/ 2928 batches | lr 2.32 | ms/batch 14.94 | loss  4.54 | ppl    93.35\n",
      "| epoch  16 |  2200/ 2928 batches | lr 2.32 | ms/batch 14.96 | loss  4.39 | ppl    80.94\n",
      "| epoch  16 |  2400/ 2928 batches | lr 2.32 | ms/batch 14.92 | loss  4.50 | ppl    90.27\n",
      "| epoch  16 |  2600/ 2928 batches | lr 2.32 | ms/batch 14.83 | loss  4.52 | ppl    91.68\n",
      "| epoch  16 |  2800/ 2928 batches | lr 2.32 | ms/batch 14.82 | loss  4.47 | ppl    87.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 45.20s | valid loss  5.56 | valid ppl   259.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2928 batches | lr 2.20 | ms/batch 14.79 | loss  4.53 | ppl    92.62\n",
      "| epoch  17 |   400/ 2928 batches | lr 2.20 | ms/batch 14.60 | loss  4.54 | ppl    93.84\n",
      "| epoch  17 |   600/ 2928 batches | lr 2.20 | ms/batch 14.68 | loss  4.38 | ppl    80.14\n",
      "| epoch  17 |   800/ 2928 batches | lr 2.20 | ms/batch 14.74 | loss  4.45 | ppl    85.61\n",
      "| epoch  17 |  1000/ 2928 batches | lr 2.20 | ms/batch 14.78 | loss  4.44 | ppl    84.90\n",
      "| epoch  17 |  1200/ 2928 batches | lr 2.20 | ms/batch 14.76 | loss  4.48 | ppl    88.59\n",
      "| epoch  17 |  1400/ 2928 batches | lr 2.20 | ms/batch 14.79 | loss  4.48 | ppl    88.51\n",
      "| epoch  17 |  1600/ 2928 batches | lr 2.20 | ms/batch 14.76 | loss  4.53 | ppl    92.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  1800/ 2928 batches | lr 2.20 | ms/batch 14.75 | loss  4.51 | ppl    90.64\n",
      "| epoch  17 |  2000/ 2928 batches | lr 2.20 | ms/batch 14.74 | loss  4.50 | ppl    89.76\n",
      "| epoch  17 |  2200/ 2928 batches | lr 2.20 | ms/batch 14.92 | loss  4.36 | ppl    78.22\n",
      "| epoch  17 |  2400/ 2928 batches | lr 2.20 | ms/batch 14.99 | loss  4.46 | ppl    86.65\n",
      "| epoch  17 |  2600/ 2928 batches | lr 2.20 | ms/batch 14.91 | loss  4.48 | ppl    88.48\n",
      "| epoch  17 |  2800/ 2928 batches | lr 2.20 | ms/batch 14.88 | loss  4.43 | ppl    84.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 45.21s | valid loss  5.53 | valid ppl   252.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2928 batches | lr 2.09 | ms/batch 14.91 | loss  4.49 | ppl    89.11\n",
      "| epoch  18 |   400/ 2928 batches | lr 2.09 | ms/batch 14.70 | loss  4.51 | ppl    90.58\n",
      "| epoch  18 |   600/ 2928 batches | lr 2.09 | ms/batch 14.67 | loss  4.35 | ppl    77.56\n",
      "| epoch  18 |   800/ 2928 batches | lr 2.09 | ms/batch 14.65 | loss  4.41 | ppl    82.13\n",
      "| epoch  18 |  1000/ 2928 batches | lr 2.09 | ms/batch 14.64 | loss  4.41 | ppl    82.09\n",
      "| epoch  18 |  1200/ 2928 batches | lr 2.09 | ms/batch 14.71 | loss  4.45 | ppl    85.20\n",
      "| epoch  18 |  1400/ 2928 batches | lr 2.09 | ms/batch 15.06 | loss  4.44 | ppl    85.18\n",
      "| epoch  18 |  1600/ 2928 batches | lr 2.09 | ms/batch 14.83 | loss  4.49 | ppl    89.19\n",
      "| epoch  18 |  1800/ 2928 batches | lr 2.09 | ms/batch 14.82 | loss  4.48 | ppl    88.03\n",
      "| epoch  18 |  2000/ 2928 batches | lr 2.09 | ms/batch 14.82 | loss  4.47 | ppl    86.96\n",
      "| epoch  18 |  2200/ 2928 batches | lr 2.09 | ms/batch 14.78 | loss  4.32 | ppl    75.47\n",
      "| epoch  18 |  2400/ 2928 batches | lr 2.09 | ms/batch 14.76 | loss  4.43 | ppl    83.78\n",
      "| epoch  18 |  2600/ 2928 batches | lr 2.09 | ms/batch 14.78 | loss  4.45 | ppl    85.28\n",
      "| epoch  18 |  2800/ 2928 batches | lr 2.09 | ms/batch 14.77 | loss  4.40 | ppl    81.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 45.19s | valid loss  5.54 | valid ppl   254.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2928 batches | lr 1.99 | ms/batch 14.90 | loss  4.45 | ppl    85.96\n",
      "| epoch  19 |   400/ 2928 batches | lr 1.99 | ms/batch 14.76 | loss  4.47 | ppl    87.70\n",
      "| epoch  19 |   600/ 2928 batches | lr 1.99 | ms/batch 14.77 | loss  4.32 | ppl    74.90\n",
      "| epoch  19 |   800/ 2928 batches | lr 1.99 | ms/batch 14.77 | loss  4.37 | ppl    79.34\n",
      "| epoch  19 |  1000/ 2928 batches | lr 1.99 | ms/batch 14.77 | loss  4.38 | ppl    79.80\n",
      "| epoch  19 |  1200/ 2928 batches | lr 1.99 | ms/batch 14.78 | loss  4.41 | ppl    82.51\n",
      "| epoch  19 |  1400/ 2928 batches | lr 1.99 | ms/batch 14.80 | loss  4.41 | ppl    82.34\n",
      "| epoch  19 |  1600/ 2928 batches | lr 1.99 | ms/batch 14.72 | loss  4.45 | ppl    85.95\n",
      "| epoch  19 |  1800/ 2928 batches | lr 1.99 | ms/batch 14.70 | loss  4.45 | ppl    85.22\n",
      "| epoch  19 |  2000/ 2928 batches | lr 1.99 | ms/batch 14.69 | loss  4.43 | ppl    84.19\n",
      "| epoch  19 |  2200/ 2928 batches | lr 1.99 | ms/batch 14.66 | loss  4.29 | ppl    72.98\n",
      "| epoch  19 |  2400/ 2928 batches | lr 1.99 | ms/batch 14.71 | loss  4.39 | ppl    80.95\n",
      "| epoch  19 |  2600/ 2928 batches | lr 1.99 | ms/batch 14.77 | loss  4.41 | ppl    82.54\n",
      "| epoch  19 |  2800/ 2928 batches | lr 1.99 | ms/batch 14.78 | loss  4.36 | ppl    78.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 45.07s | valid loss  5.57 | valid ppl   262.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2928 batches | lr 1.89 | ms/batch 14.93 | loss  4.43 | ppl    83.59\n",
      "| epoch  20 |   400/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.44 | ppl    84.83\n",
      "| epoch  20 |   600/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.28 | ppl    72.55\n",
      "| epoch  20 |   800/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.35 | ppl    77.10\n",
      "| epoch  20 |  1000/ 2928 batches | lr 1.89 | ms/batch 14.79 | loss  4.34 | ppl    76.97\n",
      "| epoch  20 |  1200/ 2928 batches | lr 1.89 | ms/batch 14.79 | loss  4.38 | ppl    80.05\n",
      "| epoch  20 |  1400/ 2928 batches | lr 1.89 | ms/batch 14.77 | loss  4.38 | ppl    79.82\n",
      "| epoch  20 |  1600/ 2928 batches | lr 1.89 | ms/batch 14.77 | loss  4.43 | ppl    83.81\n",
      "| epoch  20 |  1800/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.40 | ppl    81.83\n",
      "| epoch  20 |  2000/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.40 | ppl    81.39\n",
      "| epoch  20 |  2200/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.26 | ppl    70.49\n",
      "| epoch  20 |  2400/ 2928 batches | lr 1.89 | ms/batch 14.78 | loss  4.37 | ppl    78.70\n",
      "| epoch  20 |  2600/ 2928 batches | lr 1.89 | ms/batch 14.72 | loss  4.38 | ppl    80.20\n",
      "| epoch  20 |  2800/ 2928 batches | lr 1.89 | ms/batch 14.68 | loss  4.34 | ppl    76.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 45.15s | valid loss  5.58 | valid ppl   265.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "CPU times: user 16min 48s, sys: 10.8 s, total: 16min 59s\n",
      "Wall time: 14min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebce08aa",
   "metadata": {},
   "source": [
    "# Transformer with mlp decoder\n",
    "\n",
    "* Reference: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c90111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -int, with zeros on dialog\n",
    "    refer to https://pytorch.org/docs/stable/generated/torch.triu.html\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float=0.1, max_len: int=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        # here we also add a dropout layer for position encoding\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Tansformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        # should be replaced with TransformerDeCoder\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len\n",
    "            \n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9260dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk'])\n",
    "vocab.set_default_index(vocab['<unk'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"\n",
    "    converts raw text into a flat tensor\n",
    "    \"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab, so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides the data into bsz separate sequences, removing extra elements that wouldn't clearly fit.\n",
    "    \n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    # rounding up based on batch size\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    # transform to bsz * seq_len, then use contiguous() to return a contiguous in memory tensor \n",
    "    # containing the same data as self tensor. If self tensor is already in the specified memory format, \n",
    "    # this function returns the self tensor: refer to https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html?highlight=contiguous#torch.Tensor.contiguous\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size) # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dda41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sources: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "        \n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90be55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a36fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00687bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 14.41 | loss  8.12 | ppl  3345.60\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 11.41 | loss  6.88 | ppl   975.14\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 11.39 | loss  6.44 | ppl   625.56\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 11.42 | loss  6.31 | ppl   549.52\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 11.38 | loss  6.18 | ppl   482.50\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 11.15 | loss  6.16 | ppl   471.07\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 11.29 | loss  6.12 | ppl   454.07\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 11.26 | loss  6.11 | ppl   450.68\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 11.32 | loss  6.02 | ppl   413.18\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 11.27 | loss  6.02 | ppl   410.86\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 11.29 | loss  5.90 | ppl   364.56\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 11.31 | loss  5.97 | ppl   390.35\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 11.34 | loss  5.96 | ppl   386.68\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 11.41 | loss  5.88 | ppl   359.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 35.35s | valid loss  5.81 | valid ppl   334.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 11.40 | loss  5.86 | ppl   351.20\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 11.75 | loss  5.85 | ppl   347.62\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 11.92 | loss  5.67 | ppl   290.44\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 12.10 | loss  5.70 | ppl   300.02\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 12.01 | loss  5.66 | ppl   287.22\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 11.85 | loss  5.68 | ppl   292.73\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 11.83 | loss  5.69 | ppl   296.46\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 11.82 | loss  5.72 | ppl   303.53\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 11.83 | loss  5.65 | ppl   285.44\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 11.79 | loss  5.67 | ppl   289.24\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 11.83 | loss  5.55 | ppl   256.60\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 11.80 | loss  5.64 | ppl   280.37\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 11.83 | loss  5.64 | ppl   280.81\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 11.81 | loss  5.58 | ppl   264.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 36.17s | valid loss  5.67 | valid ppl   289.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.60 | ppl   271.02\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.62 | ppl   275.28\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 11.82 | loss  5.43 | ppl   227.11\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.48 | ppl   240.80\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 11.80 | loss  5.44 | ppl   230.73\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.48 | ppl   239.13\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 11.83 | loss  5.49 | ppl   243.40\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.52 | ppl   250.77\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 11.82 | loss  5.47 | ppl   237.09\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 11.83 | loss  5.49 | ppl   241.97\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 11.83 | loss  5.36 | ppl   212.32\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 11.83 | loss  5.47 | ppl   236.77\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 11.84 | loss  5.47 | ppl   236.90\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 11.80 | loss  5.40 | ppl   222.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 36.18s | valid loss  5.63 | valid ppl   277.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 4.29 | ms/batch 11.88 | loss  5.44 | ppl   230.13\n",
      "| epoch   4 |   400/ 2928 batches | lr 4.29 | ms/batch 11.83 | loss  5.45 | ppl   233.43\n",
      "| epoch   4 |   600/ 2928 batches | lr 4.29 | ms/batch 11.80 | loss  5.26 | ppl   192.46\n",
      "| epoch   4 |   800/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.32 | ppl   205.34\n",
      "| epoch   4 |  1000/ 2928 batches | lr 4.29 | ms/batch 11.81 | loss  5.29 | ppl   197.48\n",
      "| epoch   4 |  1200/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.33 | ppl   207.28\n",
      "| epoch   4 |  1400/ 2928 batches | lr 4.29 | ms/batch 11.80 | loss  5.35 | ppl   211.65\n",
      "| epoch   4 |  1600/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.39 | ppl   219.57\n",
      "| epoch   4 |  1800/ 2928 batches | lr 4.29 | ms/batch 11.79 | loss  5.35 | ppl   210.64\n",
      "| epoch   4 |  2000/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.35 | ppl   210.32\n",
      "| epoch   4 |  2200/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.22 | ppl   184.09\n",
      "| epoch   4 |  2400/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.33 | ppl   207.44\n",
      "| epoch   4 |  2600/ 2928 batches | lr 4.29 | ms/batch 11.81 | loss  5.34 | ppl   208.32\n",
      "| epoch   4 |  2800/ 2928 batches | lr 4.29 | ms/batch 11.84 | loss  5.27 | ppl   195.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 36.18s | valid loss  5.60 | valid ppl   269.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 4.07 | ms/batch 11.90 | loss  5.31 | ppl   203.27\n",
      "| epoch   5 |   400/ 2928 batches | lr 4.07 | ms/batch 11.80 | loss  5.34 | ppl   207.92\n",
      "| epoch   5 |   600/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.15 | ppl   172.36\n",
      "| epoch   5 |   800/ 2928 batches | lr 4.07 | ms/batch 11.82 | loss  5.22 | ppl   184.20\n",
      "| epoch   5 |  1000/ 2928 batches | lr 4.07 | ms/batch 11.83 | loss  5.17 | ppl   176.75\n",
      "| epoch   5 |  1200/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.22 | ppl   185.74\n",
      "| epoch   5 |  1400/ 2928 batches | lr 4.07 | ms/batch 11.85 | loss  5.24 | ppl   187.99\n",
      "| epoch   5 |  1600/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.28 | ppl   197.06\n",
      "| epoch   5 |  1800/ 2928 batches | lr 4.07 | ms/batch 11.80 | loss  5.23 | ppl   186.76\n",
      "| epoch   5 |  2000/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.23 | ppl   187.67\n",
      "| epoch   5 |  2200/ 2928 batches | lr 4.07 | ms/batch 11.83 | loss  5.09 | ppl   162.90\n",
      "| epoch   5 |  2400/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.22 | ppl   184.12\n",
      "| epoch   5 |  2600/ 2928 batches | lr 4.07 | ms/batch 11.81 | loss  5.22 | ppl   185.08\n",
      "| epoch   5 |  2800/ 2928 batches | lr 4.07 | ms/batch 11.84 | loss  5.16 | ppl   174.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 36.19s | valid loss  5.57 | valid ppl   262.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 3.87 | ms/batch 11.89 | loss  5.20 | ppl   181.47\n",
      "| epoch   6 |   400/ 2928 batches | lr 3.87 | ms/batch 11.78 | loss  5.22 | ppl   185.61\n",
      "| epoch   6 |   600/ 2928 batches | lr 3.87 | ms/batch 11.83 | loss  5.03 | ppl   153.26\n",
      "| epoch   6 |   800/ 2928 batches | lr 3.87 | ms/batch 11.77 | loss  5.11 | ppl   165.37\n",
      "| epoch   6 |  1000/ 2928 batches | lr 3.87 | ms/batch 11.82 | loss  5.07 | ppl   159.81\n",
      "| epoch   6 |  1200/ 2928 batches | lr 3.87 | ms/batch 11.82 | loss  5.12 | ppl   167.43\n",
      "| epoch   6 |  1400/ 2928 batches | lr 3.87 | ms/batch 11.84 | loss  5.14 | ppl   170.22\n",
      "| epoch   6 |  1600/ 2928 batches | lr 3.87 | ms/batch 11.84 | loss  5.19 | ppl   178.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1800/ 2928 batches | lr 3.87 | ms/batch 11.84 | loss  5.14 | ppl   170.26\n",
      "| epoch   6 |  2000/ 2928 batches | lr 3.87 | ms/batch 11.81 | loss  5.13 | ppl   169.44\n",
      "| epoch   6 |  2200/ 2928 batches | lr 3.87 | ms/batch 11.83 | loss  5.01 | ppl   149.19\n",
      "| epoch   6 |  2400/ 2928 batches | lr 3.87 | ms/batch 11.82 | loss  5.12 | ppl   166.66\n",
      "| epoch   6 |  2600/ 2928 batches | lr 3.87 | ms/batch 11.82 | loss  5.13 | ppl   169.61\n",
      "| epoch   6 |  2800/ 2928 batches | lr 3.87 | ms/batch 11.83 | loss  5.07 | ppl   158.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 36.16s | valid loss  5.58 | valid ppl   266.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 3.68 | ms/batch 11.88 | loss  5.11 | ppl   165.71\n",
      "| epoch   7 |   400/ 2928 batches | lr 3.68 | ms/batch 11.83 | loss  5.13 | ppl   168.80\n",
      "| epoch   7 |   600/ 2928 batches | lr 3.68 | ms/batch 11.83 | loss  4.95 | ppl   140.88\n",
      "| epoch   7 |   800/ 2928 batches | lr 3.68 | ms/batch 11.81 | loss  5.02 | ppl   151.06\n",
      "| epoch   7 |  1000/ 2928 batches | lr 3.68 | ms/batch 11.84 | loss  4.99 | ppl   146.26\n",
      "| epoch   7 |  1200/ 2928 batches | lr 3.68 | ms/batch 11.82 | loss  5.03 | ppl   153.54\n",
      "| epoch   7 |  1400/ 2928 batches | lr 3.68 | ms/batch 11.84 | loss  5.05 | ppl   156.52\n",
      "| epoch   7 |  1600/ 2928 batches | lr 3.68 | ms/batch 11.83 | loss  5.10 | ppl   163.51\n",
      "| epoch   7 |  1800/ 2928 batches | lr 3.68 | ms/batch 11.84 | loss  5.05 | ppl   156.49\n",
      "| epoch   7 |  2000/ 2928 batches | lr 3.68 | ms/batch 11.81 | loss  5.05 | ppl   156.35\n",
      "| epoch   7 |  2200/ 2928 batches | lr 3.68 | ms/batch 11.84 | loss  4.91 | ppl   135.85\n",
      "| epoch   7 |  2400/ 2928 batches | lr 3.68 | ms/batch 11.81 | loss  5.03 | ppl   153.61\n",
      "| epoch   7 |  2600/ 2928 batches | lr 3.68 | ms/batch 11.84 | loss  5.05 | ppl   155.75\n",
      "| epoch   7 |  2800/ 2928 batches | lr 3.68 | ms/batch 11.81 | loss  4.98 | ppl   145.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 36.19s | valid loss  5.50 | valid ppl   244.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 3.49 | ms/batch 11.88 | loss  5.03 | ppl   152.82\n",
      "| epoch   8 |   400/ 2928 batches | lr 3.49 | ms/batch 12.09 | loss  5.05 | ppl   155.46\n",
      "| epoch   8 |   600/ 2928 batches | lr 3.49 | ms/batch 12.15 | loss  4.86 | ppl   129.63\n",
      "| epoch   8 |   800/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.93 | ppl   138.87\n",
      "| epoch   8 |  1000/ 2928 batches | lr 3.49 | ms/batch 12.15 | loss  4.90 | ppl   134.92\n",
      "| epoch   8 |  1200/ 2928 batches | lr 3.49 | ms/batch 12.16 | loss  4.95 | ppl   141.71\n",
      "| epoch   8 |  1400/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.97 | ppl   143.93\n",
      "| epoch   8 |  1600/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  5.01 | ppl   150.37\n",
      "| epoch   8 |  1800/ 2928 batches | lr 3.49 | ms/batch 12.16 | loss  4.98 | ppl   145.01\n",
      "| epoch   8 |  2000/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.97 | ppl   144.44\n",
      "| epoch   8 |  2200/ 2928 batches | lr 3.49 | ms/batch 12.16 | loss  4.83 | ppl   125.62\n",
      "| epoch   8 |  2400/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.96 | ppl   141.93\n",
      "| epoch   8 |  2600/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.97 | ppl   143.96\n",
      "| epoch   8 |  2800/ 2928 batches | lr 3.49 | ms/batch 12.17 | loss  4.90 | ppl   134.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 37.14s | valid loss  5.53 | valid ppl   252.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 3.32 | ms/batch 11.90 | loss  4.94 | ppl   140.24\n",
      "| epoch   9 |   400/ 2928 batches | lr 3.32 | ms/batch 11.84 | loss  4.97 | ppl   144.51\n",
      "| epoch   9 |   600/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.79 | ppl   120.08\n",
      "| epoch   9 |   800/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.86 | ppl   129.53\n",
      "| epoch   9 |  1000/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.84 | ppl   126.80\n",
      "| epoch   9 |  1200/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.89 | ppl   132.86\n",
      "| epoch   9 |  1400/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.90 | ppl   134.31\n",
      "| epoch   9 |  1600/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.94 | ppl   139.93\n",
      "| epoch   9 |  1800/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.90 | ppl   134.87\n",
      "| epoch   9 |  2000/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.90 | ppl   134.87\n",
      "| epoch   9 |  2200/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.76 | ppl   116.49\n",
      "| epoch   9 |  2400/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.88 | ppl   131.76\n",
      "| epoch   9 |  2600/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.90 | ppl   133.94\n",
      "| epoch   9 |  2800/ 2928 batches | lr 3.32 | ms/batch 11.85 | loss  4.83 | ppl   125.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 36.25s | valid loss  5.48 | valid ppl   239.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 3.15 | ms/batch 11.89 | loss  4.88 | ppl   131.25\n",
      "| epoch  10 |   400/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.91 | ppl   135.12\n",
      "| epoch  10 |   600/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.72 | ppl   112.67\n",
      "| epoch  10 |   800/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.79 | ppl   120.45\n",
      "| epoch  10 |  1000/ 2928 batches | lr 3.15 | ms/batch 11.85 | loss  4.78 | ppl   118.55\n",
      "| epoch  10 |  1200/ 2928 batches | lr 3.15 | ms/batch 11.85 | loss  4.83 | ppl   124.64\n",
      "| epoch  10 |  1400/ 2928 batches | lr 3.15 | ms/batch 11.85 | loss  4.83 | ppl   125.70\n",
      "| epoch  10 |  1600/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.88 | ppl   131.16\n",
      "| epoch  10 |  1800/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.84 | ppl   126.29\n",
      "| epoch  10 |  2000/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.84 | ppl   126.38\n",
      "| epoch  10 |  2200/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.70 | ppl   109.66\n",
      "| epoch  10 |  2400/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.82 | ppl   123.87\n",
      "| epoch  10 |  2600/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.83 | ppl   124.92\n",
      "| epoch  10 |  2800/ 2928 batches | lr 3.15 | ms/batch 11.84 | loss  4.77 | ppl   117.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 36.23s | valid loss  5.47 | valid ppl   238.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2928 batches | lr 2.99 | ms/batch 11.90 | loss  4.82 | ppl   124.19\n",
      "| epoch  11 |   400/ 2928 batches | lr 2.99 | ms/batch 11.85 | loss  4.84 | ppl   126.60\n",
      "| epoch  11 |   600/ 2928 batches | lr 2.99 | ms/batch 11.84 | loss  4.67 | ppl   106.47\n",
      "| epoch  11 |   800/ 2928 batches | lr 2.99 | ms/batch 11.88 | loss  4.73 | ppl   113.67\n",
      "| epoch  11 |  1000/ 2928 batches | lr 2.99 | ms/batch 11.87 | loss  4.72 | ppl   112.03\n",
      "| epoch  11 |  1200/ 2928 batches | lr 2.99 | ms/batch 11.87 | loss  4.76 | ppl   117.11\n",
      "| epoch  11 |  1400/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.78 | ppl   118.78\n",
      "| epoch  11 |  1600/ 2928 batches | lr 2.99 | ms/batch 11.87 | loss  4.81 | ppl   123.18\n",
      "| epoch  11 |  1800/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.79 | ppl   120.24\n",
      "| epoch  11 |  2000/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.78 | ppl   119.37\n",
      "| epoch  11 |  2200/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.64 | ppl   103.39\n",
      "| epoch  11 |  2400/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.75 | ppl   116.03\n",
      "| epoch  11 |  2600/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.77 | ppl   118.06\n",
      "| epoch  11 |  2800/ 2928 batches | lr 2.99 | ms/batch 11.86 | loss  4.71 | ppl   111.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 36.29s | valid loss  5.53 | valid ppl   251.30\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   200/ 2928 batches | lr 2.84 | ms/batch 11.92 | loss  4.76 | ppl   117.13\n",
      "| epoch  12 |   400/ 2928 batches | lr 2.84 | ms/batch 11.98 | loss  4.78 | ppl   119.10\n",
      "| epoch  12 |   600/ 2928 batches | lr 2.84 | ms/batch 12.17 | loss  4.61 | ppl   100.91\n",
      "| epoch  12 |   800/ 2928 batches | lr 2.84 | ms/batch 12.19 | loss  4.68 | ppl   107.67\n",
      "| epoch  12 |  1000/ 2928 batches | lr 2.84 | ms/batch 12.17 | loss  4.67 | ppl   106.19\n",
      "| epoch  12 |  1200/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.71 | ppl   111.31\n",
      "| epoch  12 |  1400/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.72 | ppl   111.93\n",
      "| epoch  12 |  1600/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.76 | ppl   116.62\n",
      "| epoch  12 |  1800/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.73 | ppl   113.53\n",
      "| epoch  12 |  2000/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.72 | ppl   112.21\n",
      "| epoch  12 |  2200/ 2928 batches | lr 2.84 | ms/batch 12.22 | loss  4.58 | ppl    97.45\n",
      "| epoch  12 |  2400/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.69 | ppl   109.40\n",
      "| epoch  12 |  2600/ 2928 batches | lr 2.84 | ms/batch 12.18 | loss  4.72 | ppl   111.88\n",
      "| epoch  12 |  2800/ 2928 batches | lr 2.84 | ms/batch 12.14 | loss  4.66 | ppl   105.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 37.17s | valid loss  5.51 | valid ppl   248.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2928 batches | lr 2.70 | ms/batch 11.92 | loss  4.72 | ppl   111.83\n",
      "| epoch  13 |   400/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.74 | ppl   114.20\n",
      "| epoch  13 |   600/ 2928 batches | lr 2.70 | ms/batch 11.85 | loss  4.57 | ppl    96.36\n",
      "| epoch  13 |   800/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.63 | ppl   102.57\n",
      "| epoch  13 |  1000/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.62 | ppl   101.61\n",
      "| epoch  13 |  1200/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.66 | ppl   105.86\n",
      "| epoch  13 |  1400/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.66 | ppl   105.59\n",
      "| epoch  13 |  1600/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.71 | ppl   110.68\n",
      "| epoch  13 |  1800/ 2928 batches | lr 2.70 | ms/batch 11.85 | loss  4.68 | ppl   107.87\n",
      "| epoch  13 |  2000/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.67 | ppl   106.84\n",
      "| epoch  13 |  2200/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.53 | ppl    93.00\n",
      "| epoch  13 |  2400/ 2928 batches | lr 2.70 | ms/batch 11.85 | loss  4.65 | ppl   104.36\n",
      "| epoch  13 |  2600/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.67 | ppl   106.38\n",
      "| epoch  13 |  2800/ 2928 batches | lr 2.70 | ms/batch 11.86 | loss  4.61 | ppl   100.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 36.28s | valid loss  5.51 | valid ppl   247.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2928 batches | lr 2.57 | ms/batch 11.92 | loss  4.67 | ppl   106.18\n",
      "| epoch  14 |   400/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.68 | ppl   107.85\n",
      "| epoch  14 |   600/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.52 | ppl    91.58\n",
      "| epoch  14 |   800/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.58 | ppl    97.57\n",
      "| epoch  14 |  1000/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.58 | ppl    97.10\n",
      "| epoch  14 |  1200/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.62 | ppl   101.10\n",
      "| epoch  14 |  1400/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.62 | ppl   101.15\n",
      "| epoch  14 |  1600/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.66 | ppl   105.37\n",
      "| epoch  14 |  1800/ 2928 batches | lr 2.57 | ms/batch 11.85 | loss  4.64 | ppl   103.28\n",
      "| epoch  14 |  2000/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.62 | ppl   101.91\n",
      "| epoch  14 |  2200/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.48 | ppl    88.41\n",
      "| epoch  14 |  2400/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.60 | ppl    99.25\n",
      "| epoch  14 |  2600/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.62 | ppl   101.15\n",
      "| epoch  14 |  2800/ 2928 batches | lr 2.57 | ms/batch 11.86 | loss  4.56 | ppl    95.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 36.28s | valid loss  5.53 | valid ppl   252.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2928 batches | lr 2.44 | ms/batch 11.91 | loss  4.62 | ppl   101.32\n",
      "| epoch  15 |   400/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.64 | ppl   103.45\n",
      "| epoch  15 |   600/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.48 | ppl    87.87\n",
      "| epoch  15 |   800/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.54 | ppl    93.70\n",
      "| epoch  15 |  1000/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.53 | ppl    92.60\n",
      "| epoch  15 |  1200/ 2928 batches | lr 2.44 | ms/batch 11.86 | loss  4.58 | ppl    97.14\n",
      "| epoch  15 |  1400/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.57 | ppl    96.31\n",
      "| epoch  15 |  1600/ 2928 batches | lr 2.44 | ms/batch 11.86 | loss  4.61 | ppl   100.76\n",
      "| epoch  15 |  1800/ 2928 batches | lr 2.44 | ms/batch 11.86 | loss  4.60 | ppl    99.15\n",
      "| epoch  15 |  2000/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.58 | ppl    97.53\n",
      "| epoch  15 |  2200/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.44 | ppl    85.15\n",
      "| epoch  15 |  2400/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.56 | ppl    95.23\n",
      "| epoch  15 |  2600/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.57 | ppl    96.49\n",
      "| epoch  15 |  2800/ 2928 batches | lr 2.44 | ms/batch 11.85 | loss  4.52 | ppl    91.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 36.27s | valid loss  5.53 | valid ppl   252.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 2928 batches | lr 2.32 | ms/batch 11.91 | loss  4.58 | ppl    97.34\n",
      "| epoch  16 |   400/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.60 | ppl    99.02\n",
      "| epoch  16 |   600/ 2928 batches | lr 2.32 | ms/batch 11.86 | loss  4.43 | ppl    84.15\n",
      "| epoch  16 |   800/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.50 | ppl    90.05\n",
      "| epoch  16 |  1000/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.48 | ppl    88.40\n",
      "| epoch  16 |  1200/ 2928 batches | lr 2.32 | ms/batch 11.86 | loss  4.53 | ppl    93.05\n",
      "| epoch  16 |  1400/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.53 | ppl    92.34\n",
      "| epoch  16 |  1600/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.57 | ppl    96.85\n",
      "| epoch  16 |  1800/ 2928 batches | lr 2.32 | ms/batch 11.86 | loss  4.55 | ppl    95.05\n",
      "| epoch  16 |  2000/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.54 | ppl    93.86\n",
      "| epoch  16 |  2200/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.40 | ppl    81.78\n",
      "| epoch  16 |  2400/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.51 | ppl    90.92\n",
      "| epoch  16 |  2600/ 2928 batches | lr 2.32 | ms/batch 11.85 | loss  4.53 | ppl    92.93\n",
      "| epoch  16 |  2800/ 2928 batches | lr 2.32 | ms/batch 11.86 | loss  4.48 | ppl    88.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 36.27s | valid loss  5.56 | valid ppl   258.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 2928 batches | lr 2.20 | ms/batch 11.92 | loss  4.54 | ppl    93.48\n",
      "| epoch  17 |   400/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.56 | ppl    95.66\n",
      "| epoch  17 |   600/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.40 | ppl    81.15\n",
      "| epoch  17 |   800/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.46 | ppl    86.19\n",
      "| epoch  17 |  1000/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.45 | ppl    85.56\n",
      "| epoch  17 |  1200/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.49 | ppl    89.48\n",
      "| epoch  17 |  1400/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.49 | ppl    89.45\n",
      "| epoch  17 |  1600/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.53 | ppl    92.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  1800/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.52 | ppl    91.63\n",
      "| epoch  17 |  2000/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.50 | ppl    90.39\n",
      "| epoch  17 |  2200/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.36 | ppl    78.39\n",
      "| epoch  17 |  2400/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.48 | ppl    88.05\n",
      "| epoch  17 |  2600/ 2928 batches | lr 2.20 | ms/batch 11.86 | loss  4.49 | ppl    89.52\n",
      "| epoch  17 |  2800/ 2928 batches | lr 2.20 | ms/batch 11.85 | loss  4.44 | ppl    85.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 36.27s | valid loss  5.55 | valid ppl   258.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 2928 batches | lr 2.09 | ms/batch 11.92 | loss  4.50 | ppl    89.99\n",
      "| epoch  18 |   400/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.51 | ppl    91.19\n",
      "| epoch  18 |   600/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.36 | ppl    78.27\n",
      "| epoch  18 |   800/ 2928 batches | lr 2.09 | ms/batch 11.85 | loss  4.43 | ppl    83.79\n",
      "| epoch  18 |  1000/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.42 | ppl    83.15\n",
      "| epoch  18 |  1200/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.46 | ppl    86.69\n",
      "| epoch  18 |  1400/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.46 | ppl    86.45\n",
      "| epoch  18 |  1600/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.50 | ppl    89.93\n",
      "| epoch  18 |  1800/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.49 | ppl    88.87\n",
      "| epoch  18 |  2000/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.47 | ppl    87.19\n",
      "| epoch  18 |  2200/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.33 | ppl    75.83\n",
      "| epoch  18 |  2400/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.44 | ppl    84.62\n",
      "| epoch  18 |  2600/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.46 | ppl    86.22\n",
      "| epoch  18 |  2800/ 2928 batches | lr 2.09 | ms/batch 11.86 | loss  4.40 | ppl    81.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 36.29s | valid loss  5.55 | valid ppl   258.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 2928 batches | lr 1.99 | ms/batch 11.91 | loss  4.47 | ppl    87.40\n",
      "| epoch  19 |   400/ 2928 batches | lr 1.99 | ms/batch 11.85 | loss  4.48 | ppl    88.48\n",
      "| epoch  19 |   600/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.33 | ppl    75.64\n",
      "| epoch  19 |   800/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.39 | ppl    80.77\n",
      "| epoch  19 |  1000/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.39 | ppl    80.41\n",
      "| epoch  19 |  1200/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.43 | ppl    83.58\n",
      "| epoch  19 |  1400/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.43 | ppl    83.86\n",
      "| epoch  19 |  1600/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.46 | ppl    86.79\n",
      "| epoch  19 |  1800/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.45 | ppl    85.84\n",
      "| epoch  19 |  2000/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.43 | ppl    84.32\n",
      "| epoch  19 |  2200/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.30 | ppl    73.42\n",
      "| epoch  19 |  2400/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.40 | ppl    81.77\n",
      "| epoch  19 |  2600/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.42 | ppl    83.28\n",
      "| epoch  19 |  2800/ 2928 batches | lr 1.99 | ms/batch 11.86 | loss  4.37 | ppl    79.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 36.28s | valid loss  5.56 | valid ppl   259.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 2928 batches | lr 1.89 | ms/batch 11.92 | loss  4.44 | ppl    84.84\n",
      "| epoch  20 |   400/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.45 | ppl    85.78\n",
      "| epoch  20 |   600/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.30 | ppl    73.61\n",
      "| epoch  20 |   800/ 2928 batches | lr 1.89 | ms/batch 11.85 | loss  4.37 | ppl    78.69\n",
      "| epoch  20 |  1000/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.36 | ppl    78.03\n",
      "| epoch  20 |  1200/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.40 | ppl    81.22\n",
      "| epoch  20 |  1400/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.40 | ppl    81.05\n",
      "| epoch  20 |  1600/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.43 | ppl    84.08\n",
      "| epoch  20 |  1800/ 2928 batches | lr 1.89 | ms/batch 11.85 | loss  4.42 | ppl    82.83\n",
      "| epoch  20 |  2000/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.40 | ppl    81.75\n",
      "| epoch  20 |  2200/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.26 | ppl    71.09\n",
      "| epoch  20 |  2400/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.37 | ppl    79.16\n",
      "| epoch  20 |  2600/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.39 | ppl    80.75\n",
      "| epoch  20 |  2800/ 2928 batches | lr 1.89 | ms/batch 11.86 | loss  4.34 | ppl    76.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 36.29s | valid loss  5.59 | valid ppl   266.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "CPU times: user 12min 6s, sys: 1.65 s, total: 12min 8s\n",
      "Wall time: 12min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

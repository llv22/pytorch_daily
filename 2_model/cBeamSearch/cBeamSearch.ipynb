{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corporate-clinton",
   "metadata": {},
   "source": [
    "# Smart Paraphrasing Using Constrained Beam Search in NLP\n",
    "Reference:\n",
    "* https://towardsdatascience.com/smart-paraphrasing-using-constrained-beam-search-in-nlp-9af6fd046e5c\n",
    "* code in https://colab.research.google.com/drive/1Xq7g8tTXDekL9DPKQXnhxsjCr7UqyUdT?usp=sharing\n",
    "* further courses about question generation in NLP, referring to the last section of the blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rocky-honey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc72be6305546d494afc55424c99c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d6fbd931a8431b8d69ddba1ca216c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa14af64e1964c03b70e65c6b5426d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae632f0742e41f5ae3d12cebca62d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd90dbc598114142b990d6acc3ca1a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "former-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llv23/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2279: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normal beam search\n",
      "\n",
      "Original:  Supercharge your data science skills by building real world projects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 11:24:25.192802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.10.1.dylib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paraphrasedoutput: By implementing real world projects, you can improve your data science skills.\n",
      "paraphrasedoutput: By implementing real world projects, you can boost your data science skills.\n",
      "paraphrasedoutput: By implementing real world projects, you can enhance your data science skills.\n"
     ]
    }
   ],
   "source": [
    "# Copy writing example - Use a given word in paraphrasing\n",
    "context = \"Supercharge your data science skills by building real world projects.\"\n",
    "force_words = [\"improve\"]\n",
    "\n",
    "text = \"paraphrase: \"+context + \" </s>\"\n",
    "input_ids = tokenizer(text,max_length =128, padding=True, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Beam search\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=3,\n",
    "    max_length=128,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "print (\"\\nNormal beam search\\n\")\n",
    "print (\"Original: \",context)\n",
    "for beam_output in outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blind-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constrained beam search\n",
      "\n",
      "Original:  Supercharge your data science skills by building real world projects.\n",
      "paraphrasedoutput: By implementing real world projects, you can improve your data science skills.\n",
      "paraphrasedoutput: By executing real world projects, you can improve your data science skills.\n",
      "paraphrasedoutput: Build real world projects to improve your data science skills.\n"
     ]
    }
   ],
   "source": [
    "# Constrained Beam search\n",
    "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    max_length=128,\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=3,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "print (\"\\nConstrained beam search\\n\")\n",
    "print (\"Original: \",context)\n",
    "for beam_output in outputs:\n",
    "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-gospel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

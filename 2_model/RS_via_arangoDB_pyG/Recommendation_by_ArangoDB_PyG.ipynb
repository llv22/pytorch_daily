{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "golden-rolling",
   "metadata": {},
   "source": [
    "# Integrate ArangoDB with PyTorch Geometric to Build Recommendation Systems\n",
    "\n",
    "Reference:\n",
    "* https://sachinsharma9780.medium.com/integrate-arangodb-with-pytorch-geometric-to-build-recommendation-systems-dd69db688465\n",
    "* https://python.plainenglish.io/python-for-datascientist-ignoring-warnings-can-backfire-d0463cdf4364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recognized-objective",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-15 16:20:07.507889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.10.1.dylib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from arango import ArangoClient\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import requests\n",
    "import sys\n",
    "# pip install oasis -> # oasis-0.1.3 sklearn-0.0\n",
    "import oasis\n",
    "from arango import ArangoClient\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from arango import ArangoClient\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch_geometric.data import HeteroData\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shaped-banks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  1, Loading the data present in csv files to ArangoDB\n",
    "metadata_path = './sampled_movie_dataset/movies_metadata.csv'\n",
    "df = pd.read_csv(metadata_path)\n",
    "# on these rows metadata information is missing\n",
    "df = df.drop([19730, 29503, 35587])\n",
    "# sampled from links.csv file\n",
    "links_small = pd.read_csv('./sampled_movie_dataset/links_small.csv')\n",
    "# selecting tmdbId coloumn from links_small file\n",
    "links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n",
    "df['id'] = df['id'].astype('int')\n",
    "sampled_md = df[df['id'].isin(links_small)]\n",
    "sampled_md['tagline'].fillna('', inplace=True)\n",
    "sampled_md['description'] = sampled_md['overview'] + sampled_md['tagline']\n",
    "sampled_md['description'].fillna('', inplace=True)\n",
    "sampled_md = sampled_md.reset_index()\n",
    "indices = pd.Series(sampled_md.index, index=sampled_md['title'])\n",
    "ind_gen = pd.Series(sampled_md.index, index=sampled_md['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "classical-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(m_id): 9066\n"
     ]
    }
   ],
   "source": [
    "##  2, Letâ€™s Load Ratings File\n",
    "ratings_path = './sampled_movie_dataset/ratings_small.csv'\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "# performs user and movie mappings\n",
    "def node_mappings(path, index_col):\n",
    "    df = pd.read_csv(path, index_col=index_col)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "    return mapping\n",
    "\n",
    "user_mapping = node_mappings(ratings_path, index_col='userId')\n",
    "movie_mapping = node_mappings(ratings_path, index_col='movieId')\n",
    "m_id = ratings_df['movieId'].tolist()\n",
    "# all unique movie_ids present inside ratings file\n",
    "#m_id = list(set(m_id))\n",
    "m_id = list(dict.fromkeys(m_id))\n",
    "print(f'len(m_id): {len(m_id)}')\n",
    "\n",
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "id_map = pd.read_csv('./sampled_movie_dataset/links_small.csv')[['movieId', 'tmdbId']]\n",
    "id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)\n",
    "id_map.columns = ['movieId', 'id']\n",
    "# tmbdid is same (of links_small) as of id in sampled_md\n",
    "id_map = id_map.merge(sampled_md[['title', 'id']], on='id').set_index('title')\n",
    "indices_map = id_map.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3, ArangoDB Setup\n",
    "# get temporary credentials for ArangoDB on cloud\n",
    "login = oasis.getTempCredentials(tutorialName=\"MovieRecommendations\", credentialProvider=\"https://tutorials.arangodb.cloud:8529/_db/_system/tutorialDB/tutorialDB\")\n",
    "\n",
    "# Connect to the temp database\n",
    "# Please note that we use the python-arango driver as it has better support for ArangoSearch \n",
    "movie_rec_db = oasis.connect_python_arango(login)\n",
    "# url to access the ArangoDB Web UI\n",
    "print(\"https://\"+login[\"hostname\"]+\":\"+str(login[\"port\"]))\n",
    "print(\"Username: \" + login[\"username\"])\n",
    "print(\"Password: \" + login[\"password\"])\n",
    "print(\"Database: \" + login[\"dbName\"])\n",
    "# remove ids which dont have meta data information\n",
    "def remove_movies(m_id):\n",
    "    no_metadata = []\n",
    "    for idx in range(len(m_id)):\n",
    "        tmdb_id = id_map.loc[id_map['movieId'] == m_id[idx]]\n",
    "  \n",
    "        if tmdb_id.size == 0:\n",
    "            no_metadata.append(m_id[idx])\n",
    "            #print('No Meta data information at:', m_id[idx])\n",
    "    return no_metadata\n",
    "no_metadata = remove_movies(m_id)\n",
    "## remove ids which dont have meta data information\n",
    "for element in no_metadata:\n",
    "    if element in m_id:\n",
    "        print(\"ids with no metadata information:\",element)\n",
    "        m_id.remove(element)\n",
    "# create new movie_mapping dict with only m_ids having metadata information\n",
    "movie_mappings = {}\n",
    "for idx, m in enumerate(m_id):\n",
    "    movie_mappings[m] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

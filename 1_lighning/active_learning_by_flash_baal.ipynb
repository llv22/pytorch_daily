{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f9fad3",
   "metadata": {},
   "source": [
    "# Active learning made simple using Flash and BaaL\n",
    "Reference:\n",
    "* https://devblog.pytorchlightning.ai/active-learning-made-simple-using-flash-and-baal-2216df6f872c\n",
    "* https://github.com/ElementAI/baal/blob/master/experiments/pytorch_lightning/active_image_classification.py\n",
    "* preparation for baal https://python-poetry.org/docs/\n",
    "  ```bash \n",
    "  # installation \n",
    "  curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import structlog\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.active import ActiveLearningDataset, get_heuristic\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.utils.pytorch_lightning import (\n",
    "    ActiveLightningModule,\n",
    "    ResetCallback,\n",
    "    BaalTrainer,\n",
    "    BaaLDataModule\n",
    ")\n",
    "log = structlog.get_logger(\"PL testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(BaaLDataModule):\n",
    "    def __init__(self, data_root, batch_size):\n",
    "        train_transform = transforms.Compose(\n",
    "            [transforms.RandomHorizontalFlip(), transforms.ToTensor()]\n",
    "        )\n",
    "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        active_set = ActiveLearningDataset(\n",
    "            CIFAR10(data_root, train=True, transform=train_transform, download=True),\n",
    "            pool_specifics={\"transform\": test_transform}\n",
    "        )\n",
    "        self.test_set = CIFAR10(data_root, train=False, transform=test_transform, download=True)\n",
    "        super().__init__(\n",
    "            active_dataset=active_set,\n",
    "            batch_size=batch_size,\n",
    "            train_transforms=train_transform,\n",
    "            test_transforms=test_transform\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.active_dataset, self.batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.test_set, self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(ActiveLightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.name = \"VGG16\"\n",
    "        self.version = \"0.0.1\"\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # We use `patch_module` to swap Dropout modules in the model\n",
    "        # for our implementation which enables MC-Dropou\n",
    "        self.vgg16 = patch_module(vgg16(num_classes=self.hparams.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.SGD(\n",
    "            self.parameters(), lr=self.hparams.learning_rate, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        return [optimizer], []\n",
    "    \n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser):\n",
    "        parser.add_argument(\"--num_classes\", type=int, default=10)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "        parser.add_argument(\n",
    "            \"--iterations\", type=int, default=20, help=\"Number of MC-Sampling to perform\"\n",
    "        )\n",
    "        parser.add_argument(\"--replicate_in_memory\", type=bool, default=True)\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(cmd_str):\n",
    "    \"\"\"\n",
    "    parse argument and set global parameters of the program\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    parser = VGG16.add_model_specific_args(parser)\n",
    "    parser = ArgumentParser(parents=[parser], conflict_handler=\"resolve\", add_help=False)\n",
    "    parser.add_argument(\"--heuristic\", type=str, default=\"bald\", help=\"Which heuristic to use.\")\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"/tmp\", help=\"Where to store data.\")\n",
    "    parser.add_argument(\n",
    "        \"--query_size\", type=int, default=100, help=\"How many items to label per step.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--training_duration\", type=int, default=30, help=\"How many epochs per step.\"\n",
    "    )\n",
    "    parser.add_argument(\"--gpus\", type=int, default=1, help=\"How many GPUs to use.\")\n",
    "    return parser.parse_args(cmd_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd484e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pl.seed_everything(42)\n",
    "args = parse_arguments(f'--gpus {torch.cuda.device_count()}')\n",
    "print(f'use {args.gpus} gpus in system')\n",
    "# create our dataset\n",
    "datamodule = CIFAR10DataModule(args.data_root, batch_size=args.batch_size)\n",
    "datamodule.active_dataset.label_randomly(10)\n",
    "# Get out heuristic to compute uncertainty\n",
    "heuristic = get_heuristic(args.heuristic, shuffle_prop=0.0)\n",
    "# heuristic = get_heuristic(args.heuristic, shuffle_prop=0.0, reduction=\"None\")\n",
    "model = VGG16(**vars(args)) # Instantiate VGG16\n",
    "\n",
    "# Make our PL Trainer\n",
    "logger = TensorBoardLogger(save_dir=os.path.join(\"/tmp/\", \"logs\", \"active\"), name=\"CIFAR10\")\n",
    "trainer = BaalTrainer.from_argparse_args(\n",
    "    args,\n",
    "    # The weights of the model will change as it gets\n",
    "    # trained; we need to keep a copy (deepcopy) so that\n",
    "    # we can reset them.\n",
    "    callbacks=[ResetCallback(copy.deepcopy(model.state_dict()))],\n",
    "    dataset=datamodule.active_dataset,\n",
    "    max_epochs=args.training_duration,\n",
    "    logger=logger,\n",
    "    heuristic=heuristic,\n",
    "    query_size=args.query_size,\n",
    ")\n",
    "    \n",
    "AL_STEPS = 100\n",
    "for al_step in range(AL_STEPS):\n",
    "    print(f\"Step {al_step} Dataset size {len(datamodule.active_dataset)}\")\n",
    "    trainer.fit(model, datamodule=datamodule)  # Train the model on the labelled set.\n",
    "    trainer.test(model, datamodule=datamodule)  # Get test performance.\n",
    "    should_continue = trainer.step(\n",
    "        model, datamodule=datamodule\n",
    "    )  # Label the top-k most uncertain examples.\n",
    "    if not should_continue:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4ba17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
